# -*- coding: utf-8 -*-
"""Asif_Assessment_3_HIT140.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1C3oOMmsVMxBSSEqx2xW0paDfwLwBognV
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import math
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn import metrics

# Load datasets
df1 = pd.read_csv('dataset1.csv')
df2 = pd.read_csv('dataset2.csv')

# Clean df1
df1['start_time'] = pd.to_datetime(df1['start_time'], format='%d/%m/%Y %H:%M', errors='coerce')
df1['bat_landing_to_food'] = pd.to_numeric(df1['bat_landing_to_food'], errors='coerce')
df1['season'] = df1['season'].map({0: 'Winter', 1: 'Spring'})

# Clean df2
df2['time'] = pd.to_datetime(df2['time'], format='%d/%m/%Y %H:%M', errors='coerce')

# Aggregate averages  (for df1)
avg_vigilance = df1.groupby(['month', 'season'])['bat_landing_to_food'].mean().reset_index()

# Aggregate averages  (for df2)
avg_rat_minutes = df2.groupby('month')['rat_minutes'].mean().reset_index()

# Merge on month
merged_avg = pd.merge(avg_vigilance, avg_rat_minutes, on='month', suffixes=('_bat', '_rat'))

if merged_avg.empty:
    print("Warning: No overlapping data after merge. Check months in datasets.")

# Plot: Vigilance and rat minutes over months, separated by season
plt.figure(figsize=(10, 6))

# Plot for winter
winter_data = merged_avg[merged_avg['season'] == 'Winter']
plt.plot(winter_data['month'], winter_data['bat_landing_to_food'], label='Vigilance (Winter)', color='blue')
plt.plot(winter_data['month'], winter_data['rat_minutes'], label='Rat Minutes (Winter)', color='blue', linestyle='--')

# Plot for spring
spring_data = merged_avg[merged_avg['season'] == 'Spring']
plt.plot(spring_data['month'], spring_data['bat_landing_to_food'], label='Vigilance (Spring)', color='green')
plt.plot(spring_data['month'], spring_data['rat_minutes'], label='Rat Minutes (Spring)', color='green', linestyle='--')

plt.xlabel('Month')
plt.ylabel('Average Value')
plt.title('Seasonal Trends: Bat Vigilance and Rat Activity Over Months')
plt.grid(True)
plt.legend()


plt.text(1, winter_data['bat_landing_to_food'].max(), 'Higher vigilance in winter\ndue to food scarcity', color='blue')
plt.annotate('Lower rat activity in spring\n(more encounters but abundant food)', xy=(4, spring_data['rat_minutes'].mean()),
             xytext=(2, spring_data['rat_minutes'].max() + 1), arrowprops={'facecolor':'green', 'shrink':0.05})

plt.savefig('seasonal_trends.png')
plt.show()

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Load datasets
df1 = pd.read_csv('dataset1.csv')
df2 = pd.read_csv('dataset2.csv')


df1['start_time'] = pd.to_datetime(df1['start_time'], format='%d/%m/%Y %H:%M', errors='coerce')
df1['bat_landing_to_food'] = pd.to_numeric(df1['bat_landing_to_food'], errors='coerce')
df1['season'] = df1['season'].map({0: 'Winter', 1: 'Spring'})

df2['time'] = pd.to_datetime(df2['time'], format='%d/%m/%Y %H:%M', errors='coerce')

# Bin hours for merging
df1['hours_bin'] = pd.cut(df1['hours_after_sunset'], bins=range(-3, 13, 1))
df2['hours_bin'] = pd.cut(df2['hours_after_sunset'], bins=range(-3, 13, 1))

# Merge on month and hours_bin
merged_df = pd.merge(df1, df2, on=['month', 'hours_bin'], how='inner')

if merged_df.empty:
    print("Warning: No overlapping data after merge. Analyze datasets separately if needed.")

sns.set_theme(style='whitegrid')

# Scatterplot: Vigilance vs. seconds after rat, colored by season
plt.figure(figsize=(8, 6))
sns.scatterplot(data=df1, x='seconds_after_rat_arrival', y='bat_landing_to_food', hue='season')
plt.xlabel('Seconds After Rat Arrival')
plt.ylabel('Bat Landing to Food (Vigilance Time)')
plt.title('Bat Vigilance vs. Rat Presence by Season')
plt.savefig('vigilance_scatter.png')
plt.show()

# Relational plot: Faceted by season, size by reward
sns.relplot(data=merged_df, x='hours_after_sunset_x', y='bat_landing_to_food',
            hue='season', size='reward', col='risk')
plt.suptitle('Vigilance Over Hours After Sunset, by Season and Risk')
plt.savefig('vigilance_relplot.png')
plt.show()

# Pairplot: Multivariate relationships (fixed: include 'season' for hue)
pair_cols = ['bat_landing_to_food', 'seconds_after_rat_arrival', 'risk', 'reward', 'rat_minutes', 'season']
sns.pairplot(merged_df[pair_cols], hue='season')
plt.suptitle('Pairwise Relationships: Bat Behavior and Rat Activity')
plt.savefig('behavior_pairplot.png')
plt.show()

# Clean df1 (bat-focused)
df1['start_time'] = pd.to_datetime(df1['start_time'], format='%d/%m/%Y %H:%M', errors='coerce')
df1['bat_landing_to_food'] = pd.to_numeric(df1['bat_landing_to_food'], errors='coerce')
df1['seconds_after_rat_arrival'] = pd.to_numeric(df1['seconds_after_rat_arrival'], errors='coerce')
df1 = df1.dropna(subset=['bat_landing_to_food', 'seconds_after_rat_arrival'])

# Plot scatter for df1:
plt.scatter(df1['seconds_after_rat_arrival'], df1['bat_landing_to_food'])
plt.xlabel('Seconds After Rat Arrival')
plt.ylabel('Bat Landing to Food (Vigilance Time)')
plt.title('Scatterplot: Bat Vigilance vs. Rat Presence')
plt.savefig('vigilance_scatter.png')
plt.show()

# For integration:
avg_df1 = df1.groupby('month')['bat_landing_to_food'].mean().reset_index()
avg_df2 = df2.groupby('month')['rat_minutes'].mean().reset_index()

# Merge on month
merged_avg = pd.merge(avg_df1, avg_df2, on='month')
merged_avg = merged_avg.dropna()

# Plot scatter for merged: Avg vigilance vs. avg rat activity
plt.scatter(merged_avg['rat_minutes'], merged_avg['bat_landing_to_food'])
plt.xlabel('Average Rat Minutes')
plt.ylabel('Average Bat Vigilance Time')
plt.title('Scatterplot: Avg Bat Vigilance vs. Avg Rat Activity by Month')
plt.savefig('integrated_scatter.png')
plt.show()

df1 = pd.read_csv('dataset1.csv')
df2 = pd.read_csv('dataset2.csv')

df1['start_time'] = pd.to_datetime(df1['start_time'], format='%d/%m/%Y %H:%M', errors='coerce')
df1['bat_landing_to_food'] = pd.to_numeric(df1['bat_landing_to_food'], errors='coerce')
df1['seconds_after_rat_arrival'] = pd.to_numeric(df1['seconds_after_rat_arrival'], errors='coerce')
df1 = df1.dropna(subset=['bat_landing_to_food', 'seconds_after_rat_arrival'])

# Model on df1 alone (Investigation A)
x = df1[['seconds_after_rat_arrival']]
y = df1['bat_landing_to_food']

# Split
X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.4, random_state=0)

# Build and fit model
model = LinearRegression()
model.fit(X_train, y_train)

print("Intercept (b0): ", model.intercept_)
print("Coefficient (b1): ", model.coef_)

# Predict and evaluate
y_pred = model.predict(X_test)
df_pred = pd.DataFrame({"Actual": y_test, "Predicted": y_pred})
print(df_pred)

mae = metrics.mean_absolute_error(y_test, y_pred)
mse = metrics.mean_squared_error(y_test, y_pred)
rmse = math.sqrt(mse)
y_max, y_min = y_test.max(), y_test.min()
rmse_norm = rmse / (y_max - y_min) if y_max != y_min else 0

print("MAE: ", mae)
print("MSE: ", mse)
print("RMSE: ", rmse)
print("NRMSE (Normalized): ", rmse_norm)

# Baseline: Mean predictor
y_base = np.mean(y_train)
y_pred_base = [y_base] * len(y_test)
df_base_pred = pd.DataFrame({"Actual": y_test, "Predicted": y_pred_base})
print("\nBaseline Predictions:\n", df_base_pred)

mae_base = metrics.mean_absolute_error(y_test, y_pred_base)
mse_base = metrics.mean_squared_error(y_test, y_pred_base)
rmse_base = math.sqrt(mse_base)
rmse_norm_base = rmse_base / (y_max - y_min) if y_max != y_min else 0

print("Baseline MAE: ", mae_base)
print("Baseline MSE: ", mse_base)
print("Baseline RMSE: ", rmse_base)
print("Baseline NRMSE: ", rmse_norm_base)

# Integrated Model:
avg_df1 = df1.groupby('month')['bat_landing_to_food'].mean().reset_index()
avg_df2 = df2.groupby('month')['rat_minutes'].mean().reset_index()
merged_avg = pd.merge(avg_df1, avg_df2, on='month').dropna()

if len(merged_avg) > 1:
    x_int = merged_avg[['rat_minutes']]
    y_int = merged_avg['bat_landing_to_food']

    # Small data: Use full for fit
    model_int = LinearRegression()
    model_int.fit(x_int, y_int)

    print("\nIntegrated Model Intercept: ", model_int.intercept_)
    print("Integrated Model Coefficient: ", model_int.coef_)

    y_pred_int = model_int.predict(x_int)
    mae_int = metrics.mean_absolute_error(y_int, y_pred_int)
    mse_int = metrics.mean_squared_error(y_int, y_pred_int)
    rmse_int = math.sqrt(mse_int)
    y_max_int, y_min_int = y_int.max(), y_int.min()
    rmse_norm_int = rmse_int / (y_max_int - y_min_int) if y_max_int != y_min_int else 0

    print("Integrated MAE: ", mae_int)
    print("Integrated MSE: ", mse_int)
    print("Integrated RMSE: ", rmse_int)
    print("Integrated NRMSE: ", rmse_norm_int)

    # Baseline for integrated
    y_base_int = np.mean(y_int)
    y_pred_base_int = [y_base_int] * len(y_int)
    mae_base_int = metrics.mean_absolute_error(y_int, y_pred_base_int)

else:
    print("Insufficient data points after merging for integrated model.")

# For Seasonal (Investigation B):
winter_df = df1[df1['season'] == 0].dropna(subset=['bat_landing_to_food', 'seconds_after_rat_arrival'])

import pandas as pd
import matplotlib.pyplot as plt

# Load and clean
df1 = pd.read_csv('dataset1.csv')
df2 = pd.read_csv('dataset2.csv')

df1['start_time'] = pd.to_datetime(df1['start_time'], format='%d/%m/%Y %H:%M', errors='coerce')
df1['bat_landing_to_food'] = pd.to_numeric(df1['bat_landing_to_food'], errors='coerce')
df1['seconds_after_rat_arrival'] = pd.to_numeric(df1['seconds_after_rat_arrival'], errors='coerce')
df1 = df1.dropna(subset=['bat_landing_to_food', 'seconds_after_rat_arrival', 'hours_after_sunset'])

df2['time'] = pd.to_datetime(df2['time'], format='%d/%m/%Y %H:%M', errors='coerce')
df2 = df2.dropna(subset=['rat_minutes', 'rat_arrival_number', 'food_availability', 'hours_after_sunset'])

# Bin hours for merge
df1['hours_bin'] = pd.cut(df1['hours_after_sunset'], bins=range(-3, 13, 1))
df2['hours_bin'] = pd.cut(df2['hours_after_sunset'], bins=range(-3, 13, 1))

# Merge
merged_df = pd.merge(df1, df2, on=['month', 'hours_bin'], suffixes=('_bat', '_rat')).dropna()

# Visualize linearity
fig, axs = plt.subplots(2, 2, figsize=(10, 8))
fig.suptitle('Linearity Checks: Explanatory Vars vs. Bat Vigilance')

axs[0, 0].scatter(merged_df['seconds_after_rat_arrival'], merged_df['bat_landing_to_food'])
axs[0, 0].set_xlabel('Seconds After Rat Arrival')
axs[0, 0].set_ylabel('Bat Vigilance Time')

axs[0, 1].scatter(merged_df['rat_minutes'], merged_df['bat_landing_to_food'])
axs[0, 1].set_xlabel('Rat Minutes')
axs[0, 1].set_ylabel('Bat Vigilance Time')

axs[1, 0].scatter(merged_df['food_availability'], merged_df['bat_landing_to_food'])
axs[1, 0].set_xlabel('Food Availability')
axs[1, 0].set_ylabel('Bat Vigilance Time')

axs[1, 1].scatter(merged_df['hours_after_sunset_bat'], merged_df['bat_landing_to_food'])
axs[1, 1].set_xlabel('Hours After Sunset')
axs[1, 1].set_ylabel('Bat Vigilance Time')

plt.tight_layout()
plt.savefig('linearity_checks.png')
plt.show()

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import statsmodels.api as sm

# Use merged_df from above

# Select features
features = ['seconds_after_rat_arrival', 'hours_after_sunset_bat', 'risk', 'reward', 'rat_minutes', 'rat_arrival_number', 'food_availability']
X = merged_df[features]
y = merged_df['bat_landing_to_food']

# Correlation matrix
corr = X.corr()
sns.heatmap(corr, vmin=-1, vmax=1, center=0, cmap=sns.diverging_palette(20, 220, n=200), square=True, annot=True)
plt.title('Correlation Heatmap')
plt.savefig('collinearity_heatmap.png')
plt.show()

# Basic model
X_const = sm.add_constant(X)
model = sm.OLS(y, X_const).fit()
print(model.summary())

# Fix collinearity: Drop if corr >0.7
X_fixed = X.drop(['rat_arrival_number'], axis=1)
X_fixed_const = sm.add_constant(X_fixed)
model_fixed = sm.OLS(y, X_fixed_const).fit()
print(model_fixed.summary())

import pandas as pd
import numpy as np
import math
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn import metrics

# Use merged_df (as above)

features = ['seconds_after_rat_arrival', 'hours_after_sunset_bat', 'risk', 'reward', 'rat_minutes', 'rat_arrival_number', 'food_availability']
X = merged_df[features].values
y = merged_df['bat_landing_to_food'].values

# Outlier removal (assumption: remove >3 std)
y_std = np.std(y)
y_mean = np.mean(y)
y = y[(y > y_mean - 3*y_std) & (y < y_mean + 3*y_std)]
X = X[:len(y)]  # Align

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=0)

model = LinearRegression()
model.fit(X_train, y_train)

print("Intercept: ", model.intercept_)
print("Coefficients: ", model.coef_)

y_pred = model.predict(X_test)
df_pred = pd.DataFrame({"Actual": y_test, "Predicted": y_pred})
print(df_pred)

mae = metrics.mean_absolute_error(y_test, y_pred)
mse = metrics.mean_squared_error(y_test, y_pred)
rmse = math.sqrt(mse)
rmse_norm = rmse / (y.max() - y.min()) if y.max() != y.min() else 0
r2 = metrics.r2_score(y_test, y_pred)

print("MLR Performance:")
print("MAE:", mae, "MSE:", mse, "RMSE:", rmse, "NRMSE:", rmse_norm, "RÂ²:", r2)

# Baseline
y_base = np.mean(y_train)
y_pred_base = [y_base] * len(y_test)
mae_base = metrics.mean_absolute_error(y_test, y_pred_base)

print("Baseline RÂ²:", metrics.r2_score(y_test, y_pred_base))

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import statsmodels.api as sm

# Visualize before
plt.scatter(merged_df['rat_minutes'], merged_df['bat_landing_to_food'])
plt.xlabel('Rat Minutes')
plt.ylabel('Bat Vigilance')
plt.title('Before Log')
plt.show()

# Apply log
merged_df['log_rat_minutes'] = np.log(merged_df['rat_minutes'] + 1)

# After
plt.scatter(merged_df['log_rat_minutes'], merged_df['bat_landing_to_food'])
plt.xlabel('Log Rat Minutes')
plt.title('After Log')
plt.show()

# Refit with log
features_log = ['seconds_after_rat_arrival', 'hours_after_sunset_bat', 'risk', 'reward', 'log_rat_minutes', 'food_availability']
X_log = merged_df[features_log]
X_log_const = sm.add_constant(X_log)
model_log = sm.OLS(merged_df['bat_landing_to_food'], X_log_const).fit()
print(model_log.summary())

import pandas as pd
from sklearn.preprocessing import StandardScaler
import statsmodels.api as sm

# Load and clean datasets
df1 = pd.read_csv('dataset1.csv')
df2 = pd.read_csv('dataset2.csv')

df1['start_time'] = pd.to_datetime(df1['start_time'], format='%d/%m/%Y %H:%M', errors='coerce')
df1['bat_landing_to_food'] = pd.to_numeric(df1['bat_landing_to_food'], errors='coerce')
df1['seconds_after_rat_arrival'] = pd.to_numeric(df1['seconds_after_rat_arrival'], errors='coerce')
df1 = df1.dropna(subset=['bat_landing_to_food', 'seconds_after_rat_arrival', 'hours_after_sunset', 'risk', 'reward', 'month'])

df2['time'] = pd.to_datetime(df2['time'], format='%d/%m/%Y %H:%M', errors='coerce')
df2 = df2.dropna(subset=['rat_minutes', 'rat_arrival_number', 'food_availability', 'hours_after_sunset', 'month'])

# Bin hours for merging
df1['hours_bin'] = pd.cut(df1['hours_after_sunset'], bins=range(-3, 13, 1), labels=False)
df2['hours_bin'] = pd.cut(df2['hours_after_sunset'], bins=range(-3, 13, 1), labels=False)

# Merge datasets
merged_df = pd.merge(df1, df2, on=['month', 'hours_bin'], how='inner', suffixes=('_bat', '_rat')).dropna()

# Select features and response
features = ['seconds_after_rat_arrival', 'hours_after_sunset_bat', 'risk', 'reward', 'rat_minutes', 'rat_arrival_number', 'food_availability']
X = merged_df[features]
y = merged_df['bat_landing_to_food']

# Basic model
X_const = sm.add_constant(X)
model = sm.OLS(y, X_const).fit()
print("Basic Model Summary:")
print(model.summary())

# Apply Z-score standardization
scaler = StandardScaler()
X_std = scaler.fit_transform(X)
X_std_df = pd.DataFrame(X_std, index=X.index, columns=X.columns)

# Ensure alignment by resetting
X_std_df = X_std_df.reset_index(drop=True)
y = y.reset_index(drop=True)

# Rebuild model
X_std_const = sm.add_constant(X_std_df)
model_std = sm.OLS(y, X_std_const).fit()
print("\nStandardized Model Summary:")
print(model_std.summary())

import pandas as pd
from sklearn.preprocessing import PowerTransformer
import statsmodels.api as sm

# Load and clean datasets
df1 = pd.read_csv('dataset1.csv')
df2 = pd.read_csv('dataset2.csv')

# Clean df1 (bat-focused)
df1['start_time'] = pd.to_datetime(df1['start_time'], format='%d/%m/%Y %H:%M', errors='coerce')
df1['bat_landing_to_food'] = pd.to_numeric(df1['bat_landing_to_food'], errors='coerce')
df1['seconds_after_rat_arrival'] = pd.to_numeric(df1['seconds_after_rat_arrival'], errors='coerce')
df1['hours_after_sunset'] = pd.to_numeric(df1['hours_after_sunset'], errors='coerce')
df1['risk'] = pd.to_numeric(df1['risk'], errors='coerce')
df1['reward'] = pd.to_numeric(df1['reward'], errors='coerce')
df1 = df1.dropna(subset=['bat_landing_to_food', 'seconds_after_rat_arrival', 'hours_after_sunset', 'risk', 'reward', 'month'])

# Clean df2 (rat-focused)
df2['time'] = pd.to_datetime(df2['time'], format='%d/%m/%Y %H:%M', errors='coerce')
df2['rat_minutes'] = pd.to_numeric(df2['rat_minutes'], errors='coerce')
df2['rat_arrival_number'] = pd.to_numeric(df2['rat_arrival_number'], errors='coerce')
df2['food_availability'] = pd.to_numeric(df2['food_availability'], errors='coerce')
df2['hours_after_sunset'] = pd.to_numeric(df2['hours_after_sunset'], errors='coerce')
df2 = df2.dropna(subset=['rat_minutes', 'rat_arrival_number', 'food_availability', 'hours_after_sunset', 'month'])

# Bin hours for merging
df1['hours_bin'] = pd.cut(df1['hours_after_sunset'], bins=range(-3, 13, 1), labels=False)
df2['hours_bin'] = pd.cut(df2['hours_after_sunset'], bins=range(-3, 13, 1), labels=False)

# Merge datasets on month and hours_bin
merged_df = pd.merge(df1, df2, on=['month', 'hours_bin'], how='inner', suffixes=('_bat', '_rat')).dropna()

# Select features and response variable
features = ['seconds_after_rat_arrival', 'hours_after_sunset_bat', 'risk', 'reward', 'rat_minutes', 'rat_arrival_number', 'food_availability']
X = merged_df[features]
y = merged_df['bat_landing_to_food']

# Basic model without transformation
X_const = sm.add_constant(X)
model = sm.OLS(y, X_const).fit()
print("Basic Model Summary:")
print(model.summary())

# Apply PowerTransformer for Gaussian-like distribution
pt = PowerTransformer(method='yeo-johnson')  # Yeo-Johnson handles zeros/negatives
X_pt = pt.fit_transform(X)
X_pt_df = pd.DataFrame(X_pt, index=X.index, columns=X.columns)  # Preserve original index

# Ensure alignment with y
X_pt_df = X_pt_df.reset_index(drop=True)
y = y.reset_index(drop=True)

# Rebuild model with transformed variables
X_pt_const = sm.add_constant(X_pt_df)
model_pt = sm.OLS(y, X_pt_const).fit()
print("\nPower-Transformed Model Summary:")
print(model_pt.summary())