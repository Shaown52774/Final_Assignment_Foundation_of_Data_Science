# -*- coding: utf-8 -*-
"""Final Assessment

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15z3eqNOLpL9j3V3AhyySGQFBxi-FbdWa

HIT140 Foundations of Data Science | Assessment 3 - Objective 2

Group 48
Al-Amin Dhaly (S395230)
"""

# 1. Importing Libraries
import pandas as pd, numpy as np, seaborn as sns, matplotlib.pyplot as plt
from scipy import stats
import statsmodels.api as sm, statsmodels.formula.api as smf
from statsmodels.stats.outliers_influence import variance_inflation_factor
from google.colab import files
from sklearn.naive_bayes import GaussianNB
from sklearn.cluster import KMeans
from sklearn.metrics import mean_squared_error

sns.set(style='whitegrid', palette='muted'); plt.rcParams['figure.dpi']=110

# 2. Upload datasets
print("Upload dataset1.csv"); uploaded1 = files.upload()
print("Upload dataset2.csv"); uploaded2 = files.upload()
df1 = pd.read_csv(list(uploaded1.keys())[0]); df2 = pd.read_csv(list(uploaded2.keys())[0])

# 3. Cleaning & merging
for df in [df1, df2]:
    df.columns = df.columns.str.strip().str.lower().str.replace(' ','_')
    df.drop_duplicates(inplace=True)
    for c in [c for c in df.columns if 'time' in c]: df[c]=pd.to_datetime(df[c],errors='coerce')

df2['time_bin']=pd.to_datetime(df2['time']).dt.floor('30min')
df1['time_bin']=pd.to_datetime(df1['start_time']).dt.floor('30min')
merged=pd.merge(df1,df2,on='time_bin',suffixes=('_bat','_rat'))

# 4. Feature engineering
cols = merged.columns

# 1. Rat presence indicator
if all(c in cols for c in ['start_time','rat_period_start','rat_period_end']):
    merged['rat_present'] = np.where(
        (merged['start_time'] >= merged['rat_period_start']) &
        (merged['start_time'] <= merged['rat_period_end']), 1, 0)
else:
    merged['rat_present'] = 0

# 2. Time window since rat arrival
if 'seconds_after_rat_arrival' in cols:
    merged['recent_rat_window'] = pd.cut(
        merged['seconds_after_rat_arrival'],
        bins=[-np.inf, 30, 120, np.inf],
        labels=['≤30s', '30–120s', '>120s']
    )
else:
    merged['recent_rat_window'] = 'Unknown'

# 3. Hours after sunset (if available)
if 'hours_after_sunset' in cols:
    merged['after_sunset_bin'] = pd.cut(
        merged['hours_after_sunset'],
        bins=[-np.inf, 2, 4, np.inf],
        labels=['Early', 'Mid', 'Late']
    )
else:
    merged['after_sunset_bin'] = 'Unknown'

#5. Missing-value handling
print("Missing values: ", merged.isna().sum())
merged.fillna(merged.median(numeric_only=True), inplace=True)

# 6. Descriptive stats and dispersion
# Restrict calculations to numeric columns only
num = merged.select_dtypes(include=['number'])
desc = num.describe().T

# Add dispersion and shape measures
desc['skew'] = num.skew()
desc['kurtosis'] = num.kurtosis()
desc['cv'] = desc['std'] / desc['mean']

print("\nDescriptive Summary with CV:\n",
      desc[['mean', 'std', 'cv', 'skew', 'kurtosis']].round(3))

# 7. Normality checks
plt.figure(figsize=(6,4)); stats.probplot(merged['bat_landing_to_food'].dropna(), dist="norm", plot=plt)
plt.title('Q–Q Plot for Bat Landing to Food'); plt.show()
stat,p=stats.shapiro(merged['bat_landing_to_food'].dropna())
print(f"Shapiro–Wilk normality test p={p:.4f}")
merged['z_score_food']=(merged['bat_landing_to_food']-merged['bat_landing_to_food'].mean())/merged['bat_landing_to_food'].std()

# 8. Visualisations
sns.histplot(merged['bat_landing_to_food'],kde=True,color='teal')
plt.title('Distribution of Bat Landing to Food'); plt.show()
sns.boxplot(data=merged,x='season',y='bat_landing_to_food'); plt.show()
sns.heatmap(merged.select_dtypes(float).corr(),annot=True,cmap='coolwarm'); plt.title('Correlation Heatmap'); plt.show()

# === Log Transformation for Skewed Variables ===
merged['log_rat_minutes'] = np.log(merged['rat_minutes'] + 1)

plt.figure(figsize=(6,4))
plt.scatter(merged['log_rat_minutes'], merged['bat_landing_to_food'], alpha=0.7)
plt.xlabel('Log Rat Minutes')
plt.ylabel('Bat Vigilance')
plt.title('After Log Transformation: Rat Activity vs. Vigilance')
plt.show()

# Refit Model with Log Variable
features_log = ['seconds_after_rat_arrival', 'hours_after_sunset_bat', 'risk', 'reward', 'log_rat_minutes', 'food_availability']
X_log = merged[features_log]
X_log_const = sm.add_constant(X_log)
model_log = sm.OLS(merged['bat_landing_to_food'], X_log_const).fit()
print("\nOLS with Log Transformation:\n", model_log.summary())

# === Z-Score Standardization ===
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_std = scaler.fit_transform(merged[features_log])
X_std_df = pd.DataFrame(X_std, columns=features_log)

X_std_const = sm.add_constant(X_std_df)
model_std = sm.OLS(merged['bat_landing_to_food'], X_std_const).fit()

print("\nStandardized Model Summary:\n", model_std.summary())

# === Power Transformation (Yeo–Johnson) ===
from sklearn.preprocessing import PowerTransformer

pt = PowerTransformer(method='yeo-johnson')
X_pt = pt.fit_transform(merged[features_log])
X_pt_df = pd.DataFrame(X_pt, columns=features_log)

X_pt_const = sm.add_constant(X_pt_df)
model_pt = sm.OLS(merged['bat_landing_to_food'], X_pt_const).fit()

print("\nPower-Transformed Model Summary:\n", model_pt.summary())

# 9. Hypothesis testing
risk_rat=merged.loc[merged['rat_present']==1,'risk']; risk_norat=merged.loc[merged['rat_present']==0,'risk']
print(f"T-test Risk~Rat Presence p={stats.ttest_ind(risk_rat,risk_norat,nan_policy='omit').pvalue:.4f}")
# Chi-square for season vs risk
ct=pd.crosstab(merged['season'],merged['risk']); chi2,p,_,_=stats.chi2_contingency(ct)
print(f"Chi-square Season vs Risk p={p:.4f}")

#  10. Regression analyses

#Building Formula
logit_formula = 'risk ~ rat_present + seconds_after_rat_arrival + C(season)'
if 'hours_after_sunset' in merged.columns:
    logit_formula = 'risk ~ rat_present + seconds_after_rat_arrival + hours_after_sunset + C(season)'

ols_formula = 'bat_landing_to_food ~ rat_present + rat_minutes + C(season)'
if 'hours_after_sunset' in merged.columns:
    ols_formula = 'bat_landing_to_food ~ rat_present + rat_minutes + hours_after_sunset + C(season)'

# Fit logistic and linear models
logit = smf.logit(logit_formula, data=merged).fit(disp=False)
print("\nLogistic Regression Summary:\n", logit.summary())

ols = smf.ols(ols_formula, data=merged).fit()
print("\nOLS Regression Summary:\n", ols.summary())

# === Residual diagnostics & RMSE ===
plt.scatter(ols.fittedvalues, ols.resid, alpha=0.6)
plt.axhline(0, color='red', linestyle='--')
plt.title('Residual Plot')
plt.xlabel('Fitted Values')
plt.ylabel('Residuals')
plt.tight_layout()
plt.show()

# RMSE calculation
rmse = np.sqrt(mean_squared_error(ols.model.endog, ols.fittedvalues))
print(f"RMSE = {rmse:.3f}")

# === Multicollinearity (VIF) ===
X = ols.model.exog
vif = pd.DataFrame({
    'Variable': ols.model.exog_names,
    'VIF': [variance_inflation_factor(X, i) for i in range(X.shape[1])]
})
print("\nVIF Table:\n", vif)

# === Log Transformation for Skewed Variables ===
merged['log_rat_minutes'] = np.log(merged['rat_minutes'] + 1)

plt.figure(figsize=(6,4))
plt.scatter(merged['log_rat_minutes'], merged['bat_landing_to_food'], alpha=0.7)
plt.xlabel('Log Rat Minutes')
plt.ylabel('Bat Vigilance')
plt.title('After Log Transformation: Rat Activity vs. Vigilance')
plt.show()

# Refit Model with Log Variable
features_log = ['seconds_after_rat_arrival', 'hours_after_sunset_bat', 'risk', 'reward', 'log_rat_minutes', 'food_availability']
X_log = merged[features_log]
X_log_const = sm.add_constant(X_log)
model_log = sm.OLS(merged['bat_landing_to_food'], X_log_const).fit()
print("\nOLS with Log Transformation:\n", model_log.summary())

plt.figure(figsize=(7,5))
sns.pointplot(
    data=merged,
    x='season',
    y='risk',
    hue='rat_present',
    dodge=True,
    markers=['o', 's'],
    linestyles=['-', '--'],
    ci=None,
    palette='Set2'
)
plt.title('Interaction Plot – Rat × Season')
plt.xlabel('Season')
plt.ylabel('Mean Risk Level')
plt.legend(title='Rat Present', labels=['No', 'Yes'])
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

# 11. Probability tables
joint=pd.crosstab(merged['rat_present'],merged['risk'],normalize='all')
cond=pd.crosstab(merged['rat_present'],merged['risk'],normalize='index')
print("\nJoint Probabilities:\n",joint,"\nConditional Probabilities P(Risk|Rat):\n",cond)

# 12. Bayesian update
p_risk=merged['risk'].mean()
p_rat_risk=merged.loc[merged['risk']==1,'rat_present'].mean()
p_rat_not=merged.loc[merged['risk']==0,'rat_present'].mean()
posterior=(p_rat_risk*p_risk)/(p_rat_risk*p_risk+p_rat_not*(1-p_risk))
print(f"\nPosterior P(Risk|Rat)={posterior:.3f}")
plt.bar(['Prior','Posterior'],[p_risk,posterior],color=['#69b3a2','#404080']); plt.title('Bayesian Update'); plt.show()

#  13. Mini Naïve Bayes demo (synthetic)
features = []
if 'rat_minutes' in merged.columns:
    features.append('rat_minutes')
if 'hours_after_sunset' in merged.columns:
    features.append('hours_after_sunset')
elif 'seconds_after_rat_arrival' in merged.columns:
    features.append('seconds_after_rat_arrival')

if not features:
    print("No suitable numeric features found for Naïve Bayes demo.")
else:
    X = merged[features].fillna(0)
    y = merged['risk']
    gnb = GaussianNB().fit(X, y)
    print(f"Naïve Bayes (self-fit) accuracy using {features}: {round(gnb.score(X, y),3)}")

# 14. Unsupervised learning demo (K-Means clustering)

# Choose numeric features dynamically
features = []
if 'rat_minutes' in merged.columns:
    features.append('rat_minutes')
if 'hours_after_sunset' in merged.columns:
    features.append('hours_after_sunset')
elif 'seconds_after_rat_arrival' in merged.columns:
    features.append('seconds_after_rat_arrival')

# Run clustering
X_cluster = merged[features].fillna(0)
kmeans = KMeans(n_clusters=3, random_state=0).fit(X_cluster)
merged['cluster'] = kmeans.labels_

# Pick first two features for plotting
x_col, y_col = features[0], features[1]

# Plot clusters
sns.scatterplot(x=x_col, y=y_col, hue='cluster', data=merged, palette='Set2')
plt.title(f'K-Means Clusters using {x_col} and {y_col}')
plt.tight_layout()
plt.show()

# 15. Save processed outputs
merged.to_csv('processed_bat_rat_full.csv',index=False); desc.to_csv('summary_stats_full.csv')
print("\nAll analyses complete and saved.")